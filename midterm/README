This README contains a brief description of each file in the folder, and a more lengthy discussion of
each problem on the midterm. Comments on experimental results are given in this file.

Files:

pivoted_QR.py:
 Contains my code for problem 3, and the other algorithms based off pivoted QR factorization
 Running this file will run the experiments of 4(c) on your machine

weighted_least_squares.py:
  Contains code for solving the diagonal weighted least squares problem, and for reading in google spreadsheet data
  Running this file will produce the plots for Problem 2(b)

prob_4b.py:
  Running this file will produce the plot for problem 4(b)
  It contains code to read in the google spreadsheet data and compute an unwieghted fit with pivoted-QR-based least squares.

prob2.csv:
  A csv export of the google spreadsheet data for Problem 2.

Any csv or png file starting with prob4_c_: See the discussion of Problem 4(c). An output of the experiment for that problem.


Problems:

1: written

2:
  (a)
    My implementation of the weighted least squares problem for a diagonal, positive semidefinite matrix C
    is the function weighted_diag_lstsq in the file weighted_least_squares.py
  (b)
    Running weighted_least_squares.py produces four plots of least squares fits to the data from the google spreadsheet.
    One fit is unweighted, while the other three use different diagonal weight matrices.
    Recall that ra is the x-variable, and the weighted fits are supposed to weight higher ra values more heavily.
    The three weights I chose were:
      (1) C = diag(ra)
      (2) C = diag(ra^.1)
      (3) C = diag(log(ra))
    I found that the first weighting scheme produced the best fit, and that functions that increased more aggressively with ra
    in general improved the fit. I conjecture this is because taking logs suppresses larger values, and considers only
    relative differences. Thus, the unweighted and low-weighted least squares methods lead to rather large absolute deviations
    in the fit for large values of ra.

    NOTE: All fits nu = a*ra^b were of course done by taking logs, since least squares can't apply directly.
       The resulting linear regression problem log(nu) = log(a) + log(ra)*b is amenable to least squares.

3:
  My implementation is the function pivoted_QR in the file pivoted_QR.py.

  To verify correctness, I wrote a script test_pivoted_QR.py. This script computes the pivoted QR factorizations of many
  random matrices of various shapes and sizes, and verifies that all desired properties hold (that Q has orthogonal columns,
  that R is upper triangular, that AP=QR, and that the diagonal entries of R decay in magnitude)

  See the writeup for a proof of correctness. The idea is to do column pivoting with householder triangularization, and
  to select the pivot column to have maximum norm among all candidates.

  As a funny aside, I was initially inspired by the section on pivoting with Gaussian elimination,
  and wrote a version of QR factorization that at each pivot step, selects the entry of the R_k:,k: submatrix
  having the largest magnitude, and moved it into the k,k spot, using one row swap and one column swap.
  This method computes   Qn* Un ... * Q1*U1 * A * P1 * ... * Pn = R, where the U's and P's are permutation matrices.
  The U's are permutation matrices and all orthogonal, so they can be absorbed into Q, hence we can compute AP = QR.

  This method seemed to produce the desired results for my initial test-cases, and it 'looked right' so I guessed it was correct.
  I proceeded to go on and do all the other problems using this algorithm. And my results were pretty similar to the actual ones.
  Later on, I tested this algorithm more extensively, and realized that it didn't guarantee that the diagonal entries of R always
  decreased in magnitude. So after some thought I discovered the correct pivoting scheme, and proceeded to fix the code.
  Fortunately, this was as simple as changing the column selection and disabling all my hard-wrought row pivoting.
  Can I get extra credit for getting the row pivoting to work?

4:
  (a) written
  (b)
    Running the helpfully named prob_4b.py will produce a plot of the unweighted fit to the Problem 2 data.
    The fit is computed by the function pivoted_least_squares in the file pivoted_QR.py.
  (c)
    For this problem I conducted three experiments using random matrices of shapes (2n, n), (4n, n) and (8n, n).
    I compared my pivoted_least_squares function to numpy's built in linalg.lstsq.
    Each experiment produced two files: a plot of the runtimes and a csv file containing the actual times (and matrix sizes).
    The names of the files for a given experiment depend on the shape of matrices used. For example, prob4_c_2.png
    is the plot of runtimes from the experiment dealing with matrices of shape (2n, n), prob_c_4.png the plot for matrices of
    shape (4n, n).
    The following table summarizes the files.
    Experiment  Plot            Csv
    (2n,n)      prob4_c_2.png   prob4_c_2.csv
    (4n,n)      prob4_c_4.png   prob4_c_4.csv
    (8n,n)      prob4_c_8.png   prob4_c_8.csv

    I found that (surprise, surprise) my least squares solver is significantly slower than numpy's built-in solver. The
    log-log plots indicate that for small m, the relative difference in runtime bounces around, most likely to to lower-order
    terms in the algorithmic complexities of the respective algorithms. In each plot, there is a point after which both curves
    become straight lines. The lines, however, have different slope. The relative difference in time starts increasing.
    This indicates that after that point the asymptotic complexity of my algorithm is effectively greater than numpy's least squares
    solver.

    My conjecture is that numpy's built-in is cache-aware, and tries to access memory in contiguous chunks. My implementation doesn't
    factor in any considerations of memory contiguity. Thus, for sufficiently large matrices, my naive implementation
    probably is causing a lot more CPU cache misses, which effectively increases the complexity of the algorithm. Examining the graphs
    reveals that for the (2*n, n) case this transition happens for smaller m than for the (4*n, n) case, and likewise for the
    (8*n, n) case, so roughly for the same sizes of matrices.

    Another pattern I noticed was that as the ratio between m and n increases, numpy got relatively more efficient compared to my
    pivoted QR least squares solver. I'm not entirely sure why.

    I didn't find the size of matrix at which my system ran out of RAM, although I used matrices with m > 1000. My box can initialize
    10^4 x 10^4 matrices without running out of RAM (but it's pushing it). QR factorization on something that huge would have taken many hours if not days.
    In interest of time, I elected to instead speculate about caches.

5: written
